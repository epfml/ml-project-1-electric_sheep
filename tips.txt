When dealing with categorical features, especially when they represent yes/no/don't know/blank type answers, logistic regression is a suitable model for binary classification problems like yours (where the label is either true or false). Logistic regression is commonly used when the dependent variable is binary.

Here are the steps you can follow:

1. Data Preprocessing:
a. Handling Missing Values:
For the "don't know" or "blank" responses, you might consider treating them as missing values and impute them using methods such as mean imputation, median imputation, or more advanced methods like K-nearest neighbors imputation.
b. Encoding Categorical Variables:
Convert categorical features into a numerical format. You can use one-hot encoding or label encoding, depending on the nature of your categorical data. One-hot encoding is generally preferred for logistic regression as it avoids imposing ordinal relationships between categories.
2. Model Building:
a. Logistic Regression:
Train a logistic regression model on your preprocessed data. Logistic regression models the probability that a given instance belongs to a particular class.

The logistic function (sigmoid) is used to transform the linear combination of features into a probability score between 0 and 1.

The logistic regression model can be represented as:
�
(
�
=
1
)
=
1
1
+
�
−
(
�
0
+
�
1
�
1
+
�
2
�
2
+
.
.
.
+
�
�
�
�
)
P(Y=1)= 
1+e 
−(β 
0
​
 +β 
1
​
 x 
1
​
 +β 
2
​
 x 
2
​
 +...+β 
d
​
 x 
d
​
 )
 
1
​
 

3. Model Evaluation:
a. Split the Data:
Split your dataset into training and testing sets. This allows you to assess the model's performance on unseen data.
b. Evaluate Metrics:
Use appropriate evaluation metrics such as accuracy, precision, recall, F1-score, and the area under the Receiver Operating Characteristic (ROC) curve to assess the model's performance.
4. Interpretation:
a. Coefficients:
Interpret the coefficients of the logistic regression model to understand the impact of each feature on the likelihood of the positive class.
5. Fine-tuning:
a. Regularization:
Depending on the amount of data and potential overfitting, consider adding regularization terms like L1 or L2 regularization.
6. Handling Imbalanced Data:
If your classes are imbalanced, consider techniques such as oversampling, undersampling, or using class weights to handle the imbalance.
Additional Considerations:
Cross-Validation:

Use cross-validation techniques to get a more robust estimate of your model's performance.
Feature Scaling:

While logistic regression is not as sensitive to feature scaling as some other algorithms, normalizing or standardizing your features might help.
Hyperparameter Tuning:

Experiment with different hyperparameters, especially if you use regularization.
Remember to iterate on these steps based on the performance of your model and the specific characteristics of your dataset.